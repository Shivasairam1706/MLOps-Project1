{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63705cab-7db6-47be-a273-95ab95526250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import env_var\n",
    "import spark_session\n",
    "import validate\n",
    "import logging\n",
    "import logging.config\n",
    "import ingest\n",
    "import data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a556b88-53ec-47fc-aee6-061952700e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428e5dc5-91e3-4c62-8ef3-435a6a722b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('application.log', 'w').close() ## To reset the application.log for every notebook run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec1b494-75e7-4d9b-aba8-085530c4b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.fileConfig('logging.config')\n",
    "logger = logging.getLogger('root')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bc48ea-49fe-452f-ac94-0700a021698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba0fb11-426c-4d4f-b95c-c686aa65bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:30:40,674 - root - INFO - Creating Spark Session... create_spark_session Method started\n",
      "2025-02-26 19:30:40,677 - root - INFO - Calling spark object...\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyteruser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyteruser/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-969f9b32-649f-4c31-bbc5-2f5041a246a9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 199ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-969f9b32-649f-4c31-bbc5-2f5041a246a9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/02/26 19:30:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:30:44,309 - root - INFO - Validating the Spark object\n"
     ]
    }
   ],
   "source": [
    "spark = spark_session.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b08e517-ecef-4216-852d-31a578187fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec71fcc-e8f0-4dfd-bc52-34bbd8b08580",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format, header, schema, path_file = ingest.get_src_path(env_var.src_path,'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d042c44-61ba-4c14-b3ca-b8a73545e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db7bcc9-30fe-4c7f-b8ad-58398e5df42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "main_df = ingest.read_data(in_spark=spark,in_file_path=path_file,in_file_format=file_format,in_header=header,in_schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b541309-b543-403c-8089-2b8aed69dd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122be100-93e9-402d-8c0e-08a66c121193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open time: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Close time: timestamp (nullable = true)\n",
      " |-- Quote asset volume: double (nullable = true)\n",
      " |-- Number of trades: integer (nullable = true)\n",
      " |-- Taker buy base asset volume: double (nullable = true)\n",
      " |-- Taker buy quote asset volume: double (nullable = true)\n",
      " |-- Ignore: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f88c722-2cb1-4f78-ac3d-6ed37b456bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = data_prep.renm_nd_chg_typ(main_df, env_var.bitcoin_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb129ac-4fb4-44e9-a1ae-303088cf1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8968c7ab-57b1-46c8-81c4-a864e4c1794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_tm: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- vol: double (nullable = true)\n",
      " |-- close_tm: timestamp (nullable = true)\n",
      " |-- quote_asset_vol: float (nullable = true)\n",
      " |-- no_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_vol: float (nullable = true)\n",
      " |-- taker_buy_quote_asset_vol: float (nullable = true)\n",
      " |-- ignore: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781cd07c-300e-4bbb-a6a6-62e5d67cb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df =  data_prep.data_handling(main_df,date_column='open_tm', missing_strategy=\"drop\",rolling_period=4,rolling_col='close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeff9246-7ece-40d1-a5c8-dec0ec0732d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = data_prep.data_Partition(main_df, date_column='open_tm', partition_by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d03a5f-313e-42ae-88f3-c4892ee3eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_tm: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- vol: double (nullable = true)\n",
      " |-- close_tm: timestamp (nullable = true)\n",
      " |-- quote_asset_vol: float (nullable = true)\n",
      " |-- no_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_vol: float (nullable = true)\n",
      " |-- taker_buy_quote_asset_vol: float (nullable = true)\n",
      " |-- ignore: integer (nullable = true)\n",
      " |-- rolling_avg: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64d5e25-96aa-4193-8db1-0b972cad59a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 19:30:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e5cf84-c6ab-4c04-8f0d-e3918b04bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 19:30:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:30:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/26 19:31:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ingest.load_data(main_df, in_file_path=env_var.prep_path, partitioned_by='year', in_file_format='delta', in_mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd01ad34-0b9a-4cf2-886c-00d0b56b77f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|             open_tm|           timestamp|   NULL|\n",
      "|                open|              double|   NULL|\n",
      "|                high|              double|   NULL|\n",
      "|                 low|              double|   NULL|\n",
      "|               close|              double|   NULL|\n",
      "|                 vol|              double|   NULL|\n",
      "|            close_tm|           timestamp|   NULL|\n",
      "|     quote_asset_vol|               float|   NULL|\n",
      "|        no_of_trades|                 int|   NULL|\n",
      "|taker_buy_base_as...|               float|   NULL|\n",
      "|taker_buy_quote_a...|               float|   NULL|\n",
      "|              ignore|                 int|   NULL|\n",
      "|         rolling_avg|              double|   NULL|\n",
      "|                year|                 int|   NULL|\n",
      "|# Partition Infor...|                    |       |\n",
      "|          # col_name|           data_type|comment|\n",
      "|                year|                 int|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|spark_catalog.def...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|        operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2025-02-26 19:31:...|  NULL|    NULL|SET TBLPROPERTIES|{properties -> {\"...|NULL|    NULL|     NULL|          0|  Serializable|         true|                  {}|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-02-26 19:31:...|  NULL|    NULL|            WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 8, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df = ingest.create_delta_table(in_spark = spark, table_name = 'bitcoin', delta_table_path = env_var.prep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c736252-8e80-465f-b801-0ef796e1b7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db456dec-bbf9-4b9f-9f7e-47e5854c4020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table exists at /home/jupyteruser/work/data/prep_data/\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, env_var.prep_path):\n",
    "    print(f\"Delta table exists at {env_var.prep_path}\")\n",
    "else:\n",
    "    print(f\"No Delta table found at {env_var.prep_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30f376-6e6d-412c-a340-8d12d0d8bff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
