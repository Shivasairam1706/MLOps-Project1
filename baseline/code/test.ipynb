{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63705cab-7db6-47be-a273-95ab95526250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import env_var\n",
    "import spark_session\n",
    "import validate\n",
    "import logging\n",
    "import logging.config\n",
    "import ingest\n",
    "import data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a556b88-53ec-47fc-aee6-061952700e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428e5dc5-91e3-4c62-8ef3-435a6a722b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('application.log', 'w').close() ## To reset the application.log for every notebook run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec1b494-75e7-4d9b-aba8-085530c4b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.fileConfig('logging.config')\n",
    "logger = logging.getLogger('root')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bc48ea-49fe-452f-ac94-0700a021698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba0fb11-426c-4d4f-b95c-c686aa65bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-21 19:15:12,941 - root - INFO - Creating Spark Session... create_spark_session Method started\n",
      "2025-02-21 19:15:12,942 - root - INFO - Calling spark object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/21 19:15:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-21 19:15:17,496 - root - INFO - Validating the Spark object\n"
     ]
    }
   ],
   "source": [
    "spark = spark_session.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b08e517-ecef-4216-852d-31a578187fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec71fcc-e8f0-4dfd-bc52-34bbd8b08580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-21 19:15:20,631 - root - WARNING - Retrieving the file name from the path -> /home/jupyteruser/work/data\n",
      "2025-02-21 19:15:20,633 - root - WARNING - Successfully retrieved the file and path -> /home/jupyteruser/work/data/btc_15m_data_2018_to_2025.csv\n",
      "2025-02-21 19:15:20,634 - root - WARNING - Successfully retrieved the file and path -> /home/jupyteruser/work/data/btc_1h_data_2018_to_2025.csv\n",
      "2025-02-21 19:15:20,634 - root - WARNING - Path information has been read successfully of csv\n"
     ]
    }
   ],
   "source": [
    "file_format, header, inferschema, path_file = ingest.get_src_path(env_var.src_path,'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d042c44-61ba-4c14-b3ca-b8a73545e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db7bcc9-30fe-4c7f-b8ad-58398e5df42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "main_df = ingest.ingest_data(in_spark=spark,in_file_path=path_file,in_file_format=file_format,in_header=header,in_schema=inferschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b541309-b543-403c-8089-2b8aed69dd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f88c722-2cb1-4f78-ac3d-6ed37b456bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = data_prep.renm_nd_chg_typ(main_df, env_var.new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94051d5-2360-414f-aa57-dec3f4ba76db",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    # creates spark session/object\n",
    "    spark = spark_session.create_spark_session()\n",
    "    logging.info('Application done..')\n",
    "    # Reads the files from source path and assigns base parameters using the file_type\n",
    "    file_format, header, inferschema, path_file = ingest.get_src_path(env_var.src_path,'csv')\n",
    "    # Creates a dataframe using the ingest_data method of ingest.py file\n",
    "    main_df = ingest.ingest_data(in_spark=spark,in_file_path=path_file,in_file_format=file_format,in_header=header,in_schema=inferschema)\n",
    "    # Renaming the columns and changing datatypes\n",
    "    main_df = data_prep.renm_nd_chg_typ(main_df, env_var.new_cols)\n",
    "    # Stops the spark session/object\n",
    "    spark_session.stop_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8968c7ab-57b1-46c8-81c4-a864e4c1794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, open: string, high: string, low: string, close: string, Volume: string, Quote asset volume: string, Number of trades: string, Taker buy base asset volume: string, Taker buy quote asset volume: string, ignore: string, vol: string, quote_asset_vol: string, no_of_trades: string, taker_buy_base_asset_vol: string, taker_buy_quote_asset_vol: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed033d59-fe8c-4f5a-bc1f-b2163c35aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42849232-e5b5-405c-ae19-6dc5b27000b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------------+----------------------------+------+-------+---+--------+---------------+------------+------------------------+-------------------------+\n",
      "|Open time|open|high|low|close|Volume|Close time|Quote asset volume|Number of trades|Taker buy base asset volume|Taker buy quote asset volume|ignore|open_tm|vol|close_tm|quote_asset_vol|no_of_trades|taker_buy_base_asset_vol|taker_buy_quote_asset_vol|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------------+----------------------------+------+-------+---+--------+---------------+------------+------------------------+-------------------------+\n",
      "|        0|   0|   0|  0|    0|     0|         0|                 0|               0|                          0|                           0|     0|      0|  0|       0|              0|           0|                       0|                        0|\n",
      "+---------+----+----+---+-----+------+----------+------------------+----------------+---------------------------+----------------------------+------+-------+---+--------+---------------+------------+------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = main_df.select([count(when(col(c).isNull(), c)).alias(c) for c in main_df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f2ac1a6-38d3-4c0e-a207-a6b9077206ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for trends using rolling average\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(\"open_tm\").rowsBetween(-30, 30)\n",
    "main_df = main_df.withColumn(\"rolling_avg\", mean(col(\"close\")).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28031599-5abd-468a-aad8-8bdc44e75f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b90d8d-604c-4905-9970-9c4ff6c307d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f2130ae-bb5a-4754-abd4-8b584caf0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seasonality (assuming weekly seasonality)\n",
    "df = main_df.withColumn(\"week\", weekofyear(\"open_tm\"))\n",
    "seasonal_mean = df.groupBy(\"week\").agg(mean(col(\"close\")).alias(\"seasonal_mean\"))\n",
    "df = df.join(seasonal_mean, on=\"week\", how=\"left\").withColumn(\"deseasonalized_value\", col(\"close\") - col(\"seasonal_mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c3a84c4-0ba9-409c-bac7-c84fcb982617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Test Statistic: 0.0963903148234034\n",
      "p-value: 0.9658137782553056\n"
     ]
    }
   ],
   "source": [
    "# Check for stationarity using Augmented Dickey-Fuller (ADF) test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf_test = adfuller(df.select(\"deseasonalized_value\").rdd.flatMap(lambda x: x).collect())\n",
    "print(f\"ADF Test Statistic: {adf_test[0]}\")\n",
    "print(f\"p-value: {adf_test[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1559780d-5b25-484a-a63c-ac9a97bec8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: (0.0963903148234034, 0.9658137782553056, 49, 62277, {'1%': -3.4304550077806923, '5%': -2.861586411484126, '10%': -2.566794703262921}, 872129.540202223)\n"
     ]
    }
   ],
   "source": [
    "print(f\"p-value: {adf_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff9246-7ece-40d1-a5c8-dec0ec0732d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
