{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63705cab-7db6-47be-a273-95ab95526250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import env_var\n",
    "from spark_session import get_spark_object\n",
    "from validate import get_current_date\n",
    "import logging\n",
    "import logging.config\n",
    "from ingest import ingest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428e5dc5-91e3-4c62-8ef3-435a6a722b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "open('application.log', 'w').close() ## To reset the application.log for every notebook run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec1b494-75e7-4d9b-aba8-085530c4b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.fileConfig('logging.config')\n",
    "logger = logging.getLogger('root')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba0fb11-426c-4d4f-b95c-c686aa65bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To call get_spark_object() fucntion from spark_session.py to creating a new spark session \n",
    "def SparkSession():\n",
    "    try:\n",
    "        # logging the main method for creating spark session\n",
    "        logging.info('Creating Spark Session... SparkSession Method started')\n",
    "        # logging the spark status\n",
    "        logging.info('Calling spark object...')\n",
    "        out_spark = get_spark_object(env_var.env,env_var.appName)\n",
    "        # Validating the spark session\n",
    "        logging.info('Validating the Spark object')\n",
    "        # sample function to get the current date using the newly created spark session\n",
    "        get_current_date(out_spark)\n",
    "    except exception as err:\n",
    "        logging.error(\"Unable to create spark session. An error occured... please check ===> \",str(exp))\n",
    "        sys.exit(1) # Ouputs exit code 1 incase of an issue while creating SparkSession\n",
    "    \n",
    "    return out_spark # Returns the created spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec71fcc-e8f0-4dfd-bc52-34bbd8b08580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the  path and data files based on the file type from data directory\n",
    "def get_src_path(in_path,in_file_type):\n",
    "    # To get all the files present in the \"path\" w.r.t file type\n",
    "    try:\n",
    "        logging.info(\"Retrieving the file name from the path -> {}\".format(in_path))\n",
    "        for file_nm in os.listdir(in_path):\n",
    "            out_file_format = in_file_type\n",
    "            # pulling the parquet files based on the file extension\n",
    "            if file_nm.endswith('.parquet') and in_file_type == 'parquet':\n",
    "                out_header = 'False' \n",
    "                out_inferschema = env_var.inferSchema\n",
    "                out_path_file = in_path + '/' + file_nm # Concat data directory path with parquet files\n",
    "                logging.info(\"Successfully retrieved the file and path -> {}\".format(out_path_file))\n",
    "            # pulling the csv files based on the file extension\n",
    "            elif file_nm.endswith('.csv') and in_file_type == 'csv':\n",
    "                out_header = env_var.header\n",
    "                out_inferschema = env_var.inferSchema\n",
    "                out_path_file = in_path + '/' + file_nm # Concat data directory path with csv files\n",
    "                logging.info(\"Successfully retrieved the file and path -> {}\".format(out_path_file))\n",
    "                \n",
    "    except Exception as err:\n",
    "        logger.error('An error occured while retrieving the path and file names ===>', str(err))\n",
    "    \n",
    "    logging.info(\"Path information has be read successfully of {}\".format(out_file_format))\n",
    "    # return the file format and other file info\n",
    "    return out_file_format, out_header, out_inferschema, out_path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6dbbb5-49a9-4d63-b12e-8c5a9de05845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 19:38:36,671 - root - INFO - Creating Spark Session... SparkSession Method started\n",
      "2025-02-16 19:38:36,673 - root - INFO - Calling spark object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/16 19:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 19:38:39,538 - root - INFO - Validating the Spark object\n",
      "2025-02-16 19:38:41,923 - root - INFO - Application done..\n",
      "2025-02-16 19:38:41,925 - root - INFO - Retrieving the file name from the path -> /home/jupyteruser/work/data\n",
      "2025-02-16 19:38:41,935 - root - INFO - Successfully retrieved the file and path -> /home/jupyteruser/work/data/btc_15m_data_2018_to_2025.csv\n",
      "2025-02-16 19:38:41,936 - root - INFO - Successfully retrieved the file and path -> /home/jupyteruser/work/data/btc_1h_data_2018_to_2025.csv\n",
      "2025-02-16 19:38:41,937 - root - INFO - Path information has be read successfully of csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # creates spark session/object\n",
    "    spark = SparkSession()\n",
    "    logging.info('Application done..')\n",
    "    # Reads the files from source path and assigns base parameters using the file_type\n",
    "    file_format, header, inferschema, path_file = get_src_path(env_var.src_path,'csv')\n",
    "    # Creates a dataframe using the ingest_data method of ingest.py file\n",
    "    main_df = ingest_data(in_spark=spark,in_file_path=path_file,in_file_format=file_format,in_header=header,in_inferschema=inferschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abf4b6-4026-476e-8c63-41097a8b8cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
